{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import json\n",
    "from utils import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token= os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=token, base_url=\"https://chat.int.bayer.com/api/v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[Model(id='o4-mini', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='o4-mini', max_input_tokens=200000, max_output_tokens=200000, \"\n",
      " \"description='o4-mini is optimized for fast, effective reasoning with \"\n",
      " \"exceptionally efficient performance in coding and visual tasks.', \"\n",
      " \"training_data='Up to Jun 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=11.0, output_cost_per_million_token=44.0, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='o4-mini'), \"\n",
      " \"Model(id='grok-3', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='grok-3', max_input_tokens=1000000, max_output_tokens=1000000, \"\n",
      " 'description=\"Grok 3 displays significant improvements in reasoning, '\n",
      " 'mathematics, coding, world knowledge, and instruction-following tasks. Grok '\n",
      " \"3's reasoning capabilities, refined through large scale reinforcement \"\n",
      " 'learning, allow it to think for seconds to minutes, correcting errors, '\n",
      " 'exploring alternatives, and delivering accurate answers.\", '\n",
      " \"training_data='N/A', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=3.0, output_cost_per_million_token=15.0, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg']}, 'file': None, 'video': None, \"\n",
      " \"'audio': None}, model='grok-3'), Model(id='text-embedding-3-small', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='OpenAI Text \"\n",
      " \"Embedding 3 (small)', max_input_tokens=8191, max_output_tokens=8191, \"\n",
      " 'description=\"The small version of OpenAI\\'s text-embedding-3 family. An '\n",
      " 'efficient embedding model that provides a significant upgrade over its '\n",
      " 'predecessor, the text-embedding-ada-002 model. The vector dimension is '\n",
      " '1536.\", training_data=\\'Up to Sep 2021\\', model_type=\\'embedding\\', '\n",
      " 'input_cost_per_million_token=0.02, output_cost_per_million_token=None, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='text-embedding-3-small'), \"\n",
      " \"Model(id='text-embedding-3-large', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='OpenAI Text Embedding 3 (large)', \"\n",
      " 'max_input_tokens=8191, max_output_tokens=8191, description=\"The large '\n",
      " \"version of OpenAI's text-embedding-3 family. An efficient embedding model \"\n",
      " 'that provides a significant upgrade over its predecessor, the '\n",
      " 'text-embedding-ada-002 model. The vector dimension is 3072.\", '\n",
      " \"training_data='Up to Sep 2021', model_type='embedding', \"\n",
      " 'input_cost_per_million_token=0.13, output_cost_per_million_token=None, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='text-embedding-3-large'), \"\n",
      " \"Model(id='claude-sonnet-4', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude Sonnet 4', max_input_tokens=200000, \"\n",
      " \"max_output_tokens=200000, description='Claude Sonnet 4 improves on Claude \"\n",
      " 'Sonnet 3.7 across a variety of areas, especially coding. It offers frontier '\n",
      " 'performance that’s practical for most AI use cases, including user-facing AI '\n",
      " \"assistants and high-volume tasks.', training_data='N/A', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=4.0, \"\n",
      " 'output_cost_per_million_token=15.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/jpeg', 'image/png', 'image/gif', 'image/webp']}, \"\n",
      " \"'file': {'methods': ['base64', 'url'], 'mime_types': ['application/pdf']}, \"\n",
      " \"'video': None, 'audio': None}, model='claude-sonnet-4'), \"\n",
      " \"Model(id='Cohere-rerank-v3-5-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Cohere Rerank v3.5', max_input_tokens=4096, \"\n",
      " \"max_output_tokens=4096, description='Multilingual reranking model that \"\n",
      " 'improves search results through deep semantic understanding. With a '\n",
      " '4,096-token context window, it effectively handles complex queries and large '\n",
      " \"documents, providing precise outcomes.', training_data='N/A', \"\n",
      " \"model_type='rerank', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='Cohere-rerank-v3-5-mga'), \"\n",
      " \"Model(id='gemini-2.5-pro', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Gemini 2.5 Pro', max_input_tokens=1000000, \"\n",
      " \"max_output_tokens=1000000, description='Gemini 2.5 Pro is a thinking model \"\n",
      " 'designed to tackle complex problems with enhanced reasoning and code '\n",
      " 'capabilities. Features a 1M token context window and supports multimodal '\n",
      " \"inputs including audio, images, video, text, and PDF.', training_data='Up to \"\n",
      " \"Sept 2024', model_type='chat_completion', input_cost_per_million_token=1.25, \"\n",
      " 'output_cost_per_million_token=10.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/heic', \"\n",
      " \"'image/heif']}, 'file': {'methods': ['base64', 'url'], 'mime_types': \"\n",
      " \"['application/pdf']}, 'video': None, 'audio': None}, \"\n",
      " \"model='gemini-2.5-pro'), Model(id='gemini-1.5-pro-002', created=None, \"\n",
      " \"object='model', owned_by='myGenAssist', name='Gemini 2.5 Pro', \"\n",
      " \"max_input_tokens=1000000, max_output_tokens=1000000, description='Gemini 2.5 \"\n",
      " 'Pro is a thinking model designed to tackle complex problems with enhanced '\n",
      " 'reasoning and code capabilities. Features a 1M token context window and '\n",
      " \"supports multimodal inputs including audio, images, video, text, and PDF.', \"\n",
      " \"training_data='Up to Sept 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=1.25, output_cost_per_million_token=10.0, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/heic', \"\n",
      " \"'image/heif']}, 'file': {'methods': ['base64', 'url'], 'mime_types': \"\n",
      " \"['application/pdf']}, 'video': None, 'audio': None}, \"\n",
      " \"model='gemini-1.5-pro-002'), Model(id='gemini-2.5-flash', created=None, \"\n",
      " \"object='model', owned_by='myGenAssist', name='Gemini 2.5 Flash', \"\n",
      " \"max_input_tokens=1048576, max_output_tokens=65535, description='Gemini 2.5 \"\n",
      " 'Flash is the best model from Google in terms of price and performance, and '\n",
      " 'offers well-rounded capabilities. Gemini 2.5 Flash is their first Flash '\n",
      " 'model that features thinking capabilities, which lets you see the thinking '\n",
      " \"process that the model goes through when generating its response.', \"\n",
      " \"training_data='', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.3, output_cost_per_million_token=2.5, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/heic', \"\n",
      " \"'image/heif']}, 'file': {'methods': ['base64', 'url'], 'mime_types': \"\n",
      " \"['application/pdf']}, 'video': None, 'audio': None}, \"\n",
      " \"model='gemini-2.5-flash'), Model(id='gemini-2.0-flash-001', created=None, \"\n",
      " \"object='model', owned_by='myGenAssist', name='Gemini 2.0 Flash', \"\n",
      " \"max_input_tokens=1000000, max_output_tokens=1000000, description='Next-gen \"\n",
      " 'Gemini Flash. Superior speed. Multimodal generation. 1M token context '\n",
      " \"window.', training_data='Up to Sept 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.1, output_cost_per_million_token=0.4, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/heic', \"\n",
      " \"'image/heif']}, 'file': {'methods': ['base64', 'url'], 'mime_types': \"\n",
      " \"['application/pdf']}, 'video': None, 'audio': None}, \"\n",
      " \"model='gemini-2.0-flash-001'), Model(id='gpt-4-turbo-preview', created=None, \"\n",
      " \"object='model', owned_by='myGenAssist', name='GPT 4 Turbo', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='The latest \"\n",
      " 'GPT-4 model with improved instruction following, JSON mode, reproducible '\n",
      " 'outputs, parallel function calling, and more. Returns a maximum of 4,096 '\n",
      " \"output tokens.', training_data='Up to Mar 2024', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-turbo-preview'), \"\n",
      " \"Model(id='gpt-4-1106-preview', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT 4 Turbo', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='The latest GPT-4 model with improved \"\n",
      " 'instruction following, JSON mode, reproducible outputs, parallel function '\n",
      " \"calling, and more. Returns a maximum of 4,096 output tokens.', \"\n",
      " \"training_data='Up to Mar 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=None, output_cost_per_million_token=None, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-1106-preview'), \"\n",
      " \"Model(id='deepmind/bge-m3-query', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='bge-m3', max_input_tokens=8000, \"\n",
      " \"max_output_tokens=8000, description='Multilingual embedding model.', \"\n",
      " \"training_data='Not known', model_type='embedding', \"\n",
      " 'input_cost_per_million_token=None, output_cost_per_million_token=None, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='deepmind/bge-m3-query'), \"\n",
      " \"Model(id='Meta-Llama-3-1-405B-Instruct-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Llama 3.1 405B Instruct', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Latest \"\n",
      " 'version of Meta Llama 3.1 405B Instruct. Slow, very smart and '\n",
      " \"multi-lingual', training_data='Up to Dec 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=5.33, \"\n",
      " 'output_cost_per_million_token=16.0, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='Meta-Llama-3-1-405B-Instruct-mga'), \"\n",
      " \"Model(id='gpt-5', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='GPT-5', max_input_tokens=400000, max_output_tokens=128000, \"\n",
      " \"description='GPT-5 is OpenAI’s flagship reasoning and agentic model, \"\n",
      " \"supporting long context, tool use, prompt caching, and structured outputs.', \"\n",
      " \"training_data='Up to Oct 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=1.25, output_cost_per_million_token=10.0, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-5'), \"\n",
      " \"Model(id='gpt-5-nano', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='GPT-5 Nano', max_input_tokens=400000, max_output_tokens=128000, \"\n",
      " \"description='Fastest, most cost-efficient version of GPT-5.', \"\n",
      " \"training_data='Up to May 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.05, output_cost_per_million_token=0.01, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-5-nano'), \"\n",
      " \"Model(id='gpt-4o-mini', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-4o mini', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='Our affordable and intelligent small \"\n",
      " 'model for fast, lightweight tasks. GPT-4o mini is cheaper and more capable '\n",
      " \"than GPT-3.5 Turbo.', training_data='Up to Oct 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=0.15, \"\n",
      " 'output_cost_per_million_token=0.6, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o-mini'), \"\n",
      " \"Model(id='deepmind/bge-reranker-v2-m3-onnx-o4', created=None, \"\n",
      " \"object='model', owned_by='myGenAssist', name='BGE Reranker v2 M3', \"\n",
      " \"max_input_tokens=8192, max_output_tokens=8192, description='Lightweight \"\n",
      " \"reranker model with strong multilingual capabilities.', training_data='N/A', \"\n",
      " \"model_type='rerank', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='deepmind/bge-reranker-v2-m3-onnx-o4'), \"\n",
      " \"Model(id='claude-3-5-sonnet', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude 3.5 Sonnet', max_input_tokens=200000, \"\n",
      " \"max_output_tokens=200000, description='Claude 3.5 Sonnet, Anthropic’s most \"\n",
      " 'capable model in the Claude 3 series, excels in coding, customer support, '\n",
      " 'data analysis, and visual processing. It streamlines workflows and generates '\n",
      " \"high-quality, natural-sounding content.', training_data='Up to Jun 2024', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=3.0, \"\n",
      " 'output_cost_per_million_token=15.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/jpeg', 'image/png', 'image/gif', 'image/webp']}, \"\n",
      " \"'file': {'methods': ['base64', 'url'], 'mime_types': ['application/pdf']}, \"\n",
      " \"'video': None, 'audio': None}, model='claude-3-5-sonnet'), \"\n",
      " \"Model(id='gpt-4o-2024-08-06', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='gpt-4o-2024-08-06', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='GPT-4 Omni, a multimodal model with \"\n",
      " 'audio and vision capabilities. It is a successor of GPT-4 but faster and '\n",
      " \"more capable. Upgraded version supporting `structured outputs`.', \"\n",
      " \"training_data='Up to Oct 2023', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=5.0, output_cost_per_million_token=15.0, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o-2024-08-06'), \"\n",
      " \"Model(id='gpt-5-mini', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='GPT-5 Mini', max_input_tokens=272000, max_output_tokens=128000, \"\n",
      " \"description='GPT-5 Mini is a cost-efficient variant of GPT-5 offering the \"\n",
      " \"same context length at lower latency and price.', training_data='Up to May \"\n",
      " \"2024', model_type='chat_completion', input_cost_per_million_token=0.25, \"\n",
      " 'output_cost_per_million_token=2.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-5-mini'), \"\n",
      " \"Model(id='claude-opus-4', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude Opus 4', max_input_tokens=1048576, \"\n",
      " \"max_output_tokens=65536, description='Suitable for multi‑hour agent \"\n",
      " 'workflows, long‑horizon reasoning, and tool-enabled code generation and '\n",
      " \"refactoring. Optimized for complex reasoning and long context tasks.', \"\n",
      " \"training_data='', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=15.0, output_cost_per_million_token=75.0, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/jpeg', 'image/png', 'image/gif', 'image/webp']}, \"\n",
      " \"'file': {'methods': ['base64', 'url'], 'mime_types': ['application/pdf']}, \"\n",
      " \"'video': None, 'audio': None}, model='claude-opus-4'), \"\n",
      " \"Model(id='DeepSeek-R1-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='DeepSeek R1 Reasoning', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Latest \"\n",
      " \"version of DeepSeek R1 Reasoning. Fast, smart fluent in En and Zh', \"\n",
      " \"training_data='Unknown', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.0, output_cost_per_million_token=0.0, '\n",
      " \"supports_tools=False, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities=None, model='DeepSeek-R1-mga'), Model(id='o3-mini', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='o3-mini', \"\n",
      " \"max_input_tokens=200000, max_output_tokens=200000, description='Smaller and \"\n",
      " \"faster, and cheaper than o3, expected to supersede gpt-4o.', \"\n",
      " \"training_data='Up to June 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=1.1, output_cost_per_million_token=4.4, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities=None, model='o3-mini'), Model(id='o1-mini', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='o1-mini', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Smaller and \"\n",
      " 'faster, and 80% cheaper than o1-preview, performs well at code generation '\n",
      " \"and small context operations.', training_data='Up to Oct 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=3.3, \"\n",
      " 'output_cost_per_million_token=13.2, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='o1-mini'), Model(id='o1-preview', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='o1-preview', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Focused on \"\n",
      " 'advanced reasoning and solving complex problems, including math and science '\n",
      " 'tasks. Ideal for applications that require deep contextual understanding and '\n",
      " \"agentic workflows.', training_data='Up to Oct 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=16.5, \"\n",
      " 'output_cost_per_million_token=66.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='o1-preview'), Model(id='gpt-4', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='GPT-4', \"\n",
      " \"max_input_tokens=8192, max_output_tokens=8192, description='Snapshot of \"\n",
      " \"gpt-4 from June 13th 2023 with improved function calling support.', \"\n",
      " \"training_data='Up to Sep 2021', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=21.0, output_cost_per_million_token=42.0, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4'), \"\n",
      " \"Model(id='mistral-large-2402-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Mistral Large', max_input_tokens=32000, \"\n",
      " \"max_output_tokens=32000, description='Mistral Large from Mistral. Fluent in \"\n",
      " 'English, French, Spanish, German and Italian with precise instruction '\n",
      " \"following', training_data='Up to Feb 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=8.0, output_cost_per_million_token=24.0, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='mistral-large-2402-mga'), \"\n",
      " \"Model(id='cohere-cmnd-rplus-0404', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Command-R+', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='Command R+ from Cohere has the \"\n",
      " 'capability for multilingual generation evaluated in 10 languages and highly '\n",
      " \"performant RAG capabilities.', training_data='Up to Mar 2024', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=8.0, \"\n",
      " 'output_cost_per_million_token=24.0, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='cohere-cmnd-rplus-0404'), \"\n",
      " \"Model(id='gpt-35-turbo', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-3.5', max_input_tokens=16385, \"\n",
      " \"max_output_tokens=16385, description='The latest GPT-3.5 Turbo model with \"\n",
      " 'improved instruction following, JSON mode, reproducible outputs, parallel '\n",
      " \"function calling, and more. Returns a maximum of 4,096 output tokens.', \"\n",
      " \"training_data='Up to Sep 2021', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.7, output_cost_per_million_token=1.4, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-35-turbo'), \"\n",
      " \"Model(id='gpt-4-vision-preview', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-4 Turbo Vision', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='Ability to understand images, in \"\n",
      " 'addition to all other GPT-4 Turbo capabilties. Returns a maximum of 4,096 '\n",
      " \"output tokens.', training_data='Up to Apr 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=7.0, \"\n",
      " 'output_cost_per_million_token=21.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-vision-preview'), \"\n",
      " \"Model(id='deepmind/biollama-3-70b', created=None, object='model', \"\n",
      " \"owned_by='DeepMind', name='BioLlama-3 70B', max_input_tokens=8100, \"\n",
      " \"max_output_tokens=8100, description='A Bayer hosted specialized Llama-3 70B \"\n",
      " \"model that surpasses GPT-4 in many medical tasks.', training_data='April \"\n",
      " \"2024', model_type='chat_completion', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='deepmind/biollama-3-70b'), \"\n",
      " \"Model(id='gpt-4-0613', created=None, object='model', owned_by='myGenAssist', \"\n",
      " \"name='GPT-4', max_input_tokens=8192, max_output_tokens=8192, \"\n",
      " \"description='Snapshot of gpt-4 from June 13th 2023 with improved function \"\n",
      " \"calling support.', training_data='Up to Sep 2021', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=21.0, \"\n",
      " 'output_cost_per_million_token=42.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-0613'), \"\n",
      " \"Model(id='Meta-Llama-3-1-70B-Instruct-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Llama 3.1 70B Instruct', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Latest \"\n",
      " 'version of Meta Llama 3.1 70B Instruct. Reasonably Fast, smart and '\n",
      " \"multi-lingual', training_data='Up to Dec 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=2.68, \"\n",
      " 'output_cost_per_million_token=3.54, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='Meta-Llama-3-1-70B-Instruct-mga'), \"\n",
      " \"Model(id='Meta-Llama-3-1-8B-Instruct-mga', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Llama 3.1 8B Instruct', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='Latest \"\n",
      " \"version of Meta Llama 3.1 8B Instruct. Fast, cheap and multi-lingual', \"\n",
      " \"training_data='Up to Dec 2023', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.3, output_cost_per_million_token=0.61, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='Meta-Llama-3-1-8B-Instruct-mga'), \"\n",
      " \"Model(id='gpt-4-0125-preview', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-4 (0125-preview)', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='GPT-4 Turbo \"\n",
      " 'preview model intended to reduce cases of “laziness” where the model doesn’t '\n",
      " \"complete a task.', training_data='Up to Dec 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=7.0, \"\n",
      " 'output_cost_per_million_token=21.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-0125-preview'), \"\n",
      " \"Model(id='gpt-35-turbo-1106', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-3.5', max_input_tokens=16385, \"\n",
      " \"max_output_tokens=16385, description='The latest GPT-3.5 Turbo model with \"\n",
      " 'improved instruction following, JSON mode, reproducible outputs, parallel '\n",
      " \"function calling, and more. Returns a maximum of 4,096 output tokens.', \"\n",
      " \"training_data='Up to Sep 2021', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.7, output_cost_per_million_token=1.4, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-35-turbo-1106'), \"\n",
      " \"Model(id='gpt-3.5-turbo-1106', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-3.5', max_input_tokens=16385, \"\n",
      " \"max_output_tokens=16385, description='The latest GPT-3.5 Turbo model with \"\n",
      " 'improved instruction following, JSON mode, reproducible outputs, parallel '\n",
      " \"function calling, and more. Returns a maximum of 4,096 output tokens.', \"\n",
      " \"training_data='Up to Sep 2021', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.7, output_cost_per_million_token=1.4, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-3.5-turbo-1106'), \"\n",
      " \"Model(id='gpt-3.5-turbo', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-3.5', max_input_tokens=16385, \"\n",
      " \"max_output_tokens=16385, description='The latest GPT-3.5 Turbo model with \"\n",
      " 'improved instruction following, JSON mode, reproducible outputs, parallel '\n",
      " \"function calling, and more. Returns a maximum of 4,096 output tokens.', \"\n",
      " \"training_data='Up to Sep 2021', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.7, output_cost_per_million_token=1.4, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-3.5-turbo'), \"\n",
      " \"Model(id='deepmind/meta-llama-3-8b-instruct', created=None, object='model', \"\n",
      " \"owned_by='DeepMind', name='Llama-3 8B', max_input_tokens=6000, \"\n",
      " \"max_output_tokens=6000, description='An open-source LLM recently developed \"\n",
      " \"and trained by META AI. Locally hosted in Bayer.', training_data='Up to Mar \"\n",
      " \"2024', model_type='chat_completion', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='deepmind/meta-llama-3-8b-instruct'), \"\n",
      " \"Model(id='deepmind/meta-llama-3-70b-instruct', created=None, object='model', \"\n",
      " \"owned_by='DeepMind', name='Llama-3 70B', max_input_tokens=4096, \"\n",
      " \"max_output_tokens=4096, description='An open-source LLM recently developed \"\n",
      " 'and trained by META AI. Locally hosted in Bayer, more powerful than 7B '\n",
      " \"version but slower.', training_data='Up to Mar 2024', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='deepmind/meta-llama-3-70b-instruct'), \"\n",
      " \"Model(id='claude-opus-4.1', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude Opus 4.1', max_input_tokens=1048576, \"\n",
      " \"max_output_tokens=65536, description='Successor to Opus 4 with improved \"\n",
      " \"accuracy, multi-file coding, and better agent performance.', \"\n",
      " \"training_data='', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=15.0, output_cost_per_million_token=75.0, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/jpeg', 'image/png', 'image/gif', 'image/webp']}, \"\n",
      " \"'file': {'methods': ['base64', 'url'], 'mime_types': ['application/pdf']}, \"\n",
      " \"'video': None, 'audio': None}, model='claude-opus-4.1'), \"\n",
      " \"Model(id='gpt-oss-120b', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT‑OSS‑120B', max_input_tokens=131072, \"\n",
      " \"max_output_tokens=65536, description='OpenAI’s 117B‑parameter open‑weight \"\n",
      " 'Mixture‑of‑Experts model—Apache\\\\u202f2.0 licensed, high‑reasoning, '\n",
      " \"chain‑of‑thought, agentic.', training_data='', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=0.15, output_cost_per_million_token=0.6, '\n",
      " \"supports_tools=True, supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities=None, model='gpt-oss-120b'), \"\n",
      " \"Model(id='claude-sonnet-4.5', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude Sonnet 4.5', max_input_tokens=1048576, \"\n",
      " \"max_output_tokens=65536, description='The best model for building complex \"\n",
      " 'agents that can work independently for extended periods. It advances the '\n",
      " 'frontier in coding capabilities, achieves state-of-the-art performance in '\n",
      " 'computer use, and excels at powering agents for financial analysis, '\n",
      " \"cybersecurity, and research applications..', training_data='Up to Jan 2025', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=3.0, \"\n",
      " 'output_cost_per_million_token=15.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities=None, model='claude-sonnet-4.5'), Model(id='gpt-4o', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='GPT 4o', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='GPT-4o \"\n",
      " '(Omni), a multimodal model with audio and vision capabilities. It is a '\n",
      " \"successor of GPT-4 but faster and more capable.', training_data='Up to Oct \"\n",
      " \"2023', model_type='chat_completion', input_cost_per_million_token=None, \"\n",
      " 'output_cost_per_million_token=None, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o'), \"\n",
      " \"Model(id='claude-3-7-sonnet', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='Claude 3.7 Sonnet', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='Claude 3.7 Sonnet is the most \"\n",
      " 'intelligent model from Anthropic to date and the first Claude model to offer '\n",
      " 'extended thinking—the ability to solve complex problems with careful, '\n",
      " \"step-by-step reasoning.', training_data='Up to Jun 2024', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=3.0, \"\n",
      " 'output_cost_per_million_token=15.0, supports_tools=True, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/jpeg', 'image/png', 'image/gif', 'image/webp']}, \"\n",
      " \"'file': {'methods': ['base64', 'url'], 'mime_types': ['application/pdf']}, \"\n",
      " \"'video': None, 'audio': None}, model='claude-3-7-sonnet'), \"\n",
      " \"Model(id='deepmind/medcpt', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='MedCPT Reranker', max_input_tokens=512, \"\n",
      " \"max_output_tokens=512, description='Lightweight reranker trained on PubMed \"\n",
      " \"query-article pairs.', training_data='N/A', model_type='rerank', \"\n",
      " 'input_cost_per_million_token=None, output_cost_per_million_token=None, '\n",
      " \"supports_tools=False, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities=None, model='deepmind/medcpt'), \"\n",
      " \"Model(id='gpt-4o-mini-batch', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT 4o mini Batch', max_input_tokens=1000000, \"\n",
      " \"max_output_tokens=1000000, description='A batch variant of the GPT-4O-mini \"\n",
      " \"model designed for efficient processing.', training_data='Up to July 2024', \"\n",
      " \"model_type='batch_chat_completion', input_cost_per_million_token=0.075, \"\n",
      " 'output_cost_per_million_token=0.3, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o-mini-batch'), \"\n",
      " \"Model(id='gpt-4o-batch', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT 4o Batch', max_input_tokens=1000000, \"\n",
      " \"max_output_tokens=1000000, description='A batch variant of the GPT-4O model \"\n",
      " \"designed for efficient processing.', training_data='Up to November 2024', \"\n",
      " \"model_type='batch_chat_completion', input_cost_per_million_token=1.25, \"\n",
      " 'output_cost_per_million_token=5.0, supports_tools=False, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o-batch'), \"\n",
      " \"Model(id='o3-mini-batch', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='o3 mini Batch', max_input_tokens=1000000, \"\n",
      " \"max_output_tokens=1000000, description='Optimized mini-batch model for high \"\n",
      " \"efficiency and performance.', training_data='Up to January 2025', \"\n",
      " \"model_type='batch_chat_completion', input_cost_per_million_token=0.55, \"\n",
      " 'output_cost_per_million_token=2.2, supports_tools=False, '\n",
      " \"supports_reasoning=True, model_status='available', \"\n",
      " \"supported_modalities=None, model='o3-mini-batch'), \"\n",
      " \"Model(id='gpt-4o-non-ptu', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-4o', max_input_tokens=128000, \"\n",
      " \"max_output_tokens=128000, description='GPT-4 Omni, a multimodal model with \"\n",
      " 'audio and vision capabilities. It is a successor of GPT-4 but faster and '\n",
      " \"more capable.', training_data='Up to Oct 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=5.0, \"\n",
      " 'output_cost_per_million_token=15.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4o-non-ptu'), \"\n",
      " \"Model(id='gpt-4-turbo', created=None, object='model', \"\n",
      " \"owned_by='myGenAssist', name='GPT-4 (0125-preview)', \"\n",
      " \"max_input_tokens=128000, max_output_tokens=128000, description='GPT-4 Turbo \"\n",
      " 'preview model intended to reduce cases of “laziness” where the model doesn’t '\n",
      " \"complete a task.', training_data='Up to Dec 2023', \"\n",
      " \"model_type='chat_completion', input_cost_per_million_token=7.0, \"\n",
      " 'output_cost_per_million_token=21.0, supports_tools=True, '\n",
      " \"supports_reasoning=False, model_status='unavailable', \"\n",
      " \"supported_modalities=None, model='gpt-4-turbo'), Model(id='gpt-4.1', \"\n",
      " \"created=None, object='model', owned_by='myGenAssist', name='gpt-4.1', \"\n",
      " \"max_input_tokens=1047576, max_output_tokens=1047576, description='The latest \"\n",
      " 'iteration of the GPT-4o model, trained to excel at coding and '\n",
      " 'instruction-following tasks. This model will improve the quality of agentic '\n",
      " 'workflows and accelerate the productivity of developers across all '\n",
      " \"scenarios.', training_data='Up to Jun 2024', model_type='chat_completion', \"\n",
      " 'input_cost_per_million_token=2.0, output_cost_per_million_token=8.0, '\n",
      " \"supports_tools=True, supports_reasoning=False, model_status='available', \"\n",
      " \"supported_modalities={'text': True, 'image': {'methods': ['base64', 'url'], \"\n",
      " \"'mime_types': ['image/png', 'image/jpeg', 'image/webp', 'image/gif']}, \"\n",
      " \"'file': None, 'video': None, 'audio': None}, model='gpt-4.1')]\")\n",
      "'Total number of available models: 53'\n"
     ]
    }
   ],
   "source": [
    "all_models = client.models.list()\n",
    "\n",
    "pprint(f\"{(all_models.data)}\")\n",
    "pprint(f\"Total number of available models: {len(all_models.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_database(db_path):\n",
    "    \"\"\"Explore database schema\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"📊 Database: {db_path}\")\n",
    "    print(f\"📋 Tables found: {len(tables)}\\n\")\n",
    "    \n",
    "    schema = {}\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = cursor.fetchall()\n",
    "        schema[table] = [col[1] for col in columns]\n",
    "        print(f\"  • {table}: {', '.join(schema[table][:5])}{'...' if len(schema[table]) > 5 else ''}\")\n",
    "    \n",
    "    conn.close()\n",
    "    return schema\n",
    "\n",
    "# Explore chinook database\n",
    "schema = explore_database(\"../data/chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = str(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_usage=calculate_context_percentage(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"OPEENAI_API_KEY\")\n",
    "model = 'o4-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"https://chat.int.bayer.com/api/v2\",\n",
    "    openai_api_key=token,\n",
    "    model=model,\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataAnalysisAgent:\n",
    "    \"\"\"Analyzes data using LLM-generated SQL\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path, llm, schema):\n",
    "        self.db_path = db_path\n",
    "        self.llm = llm\n",
    "        self.schema = schema\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        \n",
    "    def analyze(self, user_query: str, allowed_tables: list = None) -> dict:\n",
    "        \"\"\"Main analysis method\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 Query: {user_query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        sql_query = self._generate_sql(user_query, allowed_tables)\n",
    "        print(f\"\\n Generated SQL:\\n{sql_query}\\n\")\n",
    "        \n",
    "        if self._is_safe_query(sql_query):\n",
    "            try:\n",
    "                df = pd.read_sql_query(sql_query, self.conn)\n",
    "                print(f\" Query executed: {len(df)} rows returned\\n\")\n",
    "                \n",
    "                insights = self._generate_insights(user_query, df)\n",
    "                \n",
    "                return {\n",
    "                    \"status\": \"success\",\n",
    "                    \"query\": user_query,\n",
    "                    \"sql\": sql_query,\n",
    "                    \"data\": df,\n",
    "                    \"insights\": insights,\n",
    "                    \"rows\": len(df)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"query\": user_query\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"Unsafe query detected\",\n",
    "                \"query\": user_query\n",
    "            }\n",
    "    \n",
    "    def _generate_sql(self, user_query: str, allowed_tables: list = None) -> str:\n",
    "        \"\"\"Generate SQL from natural language\"\"\"\n",
    "        tables_info = \"\\n\".join([\n",
    "            f\"- {table}: {', '.join(cols)}\"\n",
    "            for table, cols in self.schema.items()\n",
    "            if allowed_tables is None or table in allowed_tables\n",
    "        ])\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are an expert SQL generator for SQLite databases.           \n",
    "                DATABASE SCHEMA:\n",
    "                {schema}\n",
    "                USER QUERY: {query}\n",
    "                Generate a safe, efficient SELECT query. Rules:\n",
    "                1. ONLY use SELECT statements (no INSERT, UPDATE, DELETE, DROP)\n",
    "                2. Include LIMIT clause if not specified (default LIMIT 100)\n",
    "                3. Use proper JOINs when needed\n",
    "                4. Return ONLY the SQL query, no explanations\n",
    "                SQL Query:\"\"\")\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format(schema=tables_info, query=user_query))\n",
    "        \n",
    "        # Clean the response\n",
    "        sql = response.content.strip()\n",
    "        # Remove markdown code blocks if present\n",
    "        sql = sql.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        return sql\n",
    "    \n",
    "    def _is_safe_query(self, sql: str) -> bool:\n",
    "        \"\"\"Check if SQL query is safe\"\"\"\n",
    "        sql_upper = sql.upper()\n",
    "        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']\n",
    "        \n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                print(f\"⚠️  Dangerous keyword detected: {keyword}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _generate_insights(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate natural language insights\"\"\"\n",
    "        data_summary = df.head(10).to_string()\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Based on this query and results, provide 2-3 key insights in bullet points.\n",
    "                QUERY: {query}\n",
    "                RESULTS (first 10 rows):\n",
    "                {data}\n",
    "                Key Insights (2-3 bullets):\"\"\")\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format(query=query, data=data_summary))\n",
    "        \n",
    "        return response.content.strip()\n",
    "    \n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "\n",
    "analysis_agent = DataAnalysisAgent(\"../data/chinook.db\", llm, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_agent.analyze(\"Give me top 10 artist with most albulmns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company Style Configuration for plottinng and visualziations\n",
    "COMPANY_STYLE = {\n",
    "    \"colors\": [\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\"],\n",
    "    \"font_size\": 12,\n",
    "    \"figure_size\": (10, 6),\n",
    "    \"dpi\": 100\n",
    "}\n",
    "\n",
    "sns.set_palette(COMPANY_STYLE[\"colors\"])\n",
    "plt.rcParams['figure.figsize'] = COMPANY_STYLE[\"figure_size\"]\n",
    "plt.rcParams['figure.dpi'] = COMPANY_STYLE[\"dpi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationAgent:\n",
    "    \"\"\"Creates visualizations with company branding\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, style_config=COMPANY_STYLE):\n",
    "        self.llm = llm\n",
    "        self.style = style_config\n",
    "        \n",
    "    def visualize(self, data: pd.DataFrame, query: str, output_dir=\"../outputs\") -> dict:\n",
    "        \"\"\"Create appropriate visualization\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Creating Visualization\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        chart_type = self._determine_chart_type(data, query)\n",
    "        print(f\" Chart type: {chart_type}\\n\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=self.style['figure_size'])\n",
    "        \n",
    "        if chart_type == 'bar':\n",
    "            self._create_bar_chart(data, ax)\n",
    "        elif chart_type == 'line':\n",
    "            self._create_line_chart(data, ax)\n",
    "        elif chart_type == 'pie':\n",
    "            self._create_pie_chart(data, ax)\n",
    "        elif chart_type == 'scatter':\n",
    "            self._create_scatter_plot(data, ax)\n",
    "        else:\n",
    "            self._create_bar_chart(data, ax)  # default\n",
    "        \n",
    "        # Apply company branding\n",
    "        self._apply_branding(ax, query)\n",
    "        \n",
    "        # Save\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"viz_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath, dpi=self.style['dpi'], bbox_inches='tight')\n",
    "        \n",
    "        print(f\"Saved: {filepath}\\n\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"chart_type\": chart_type,\n",
    "            \"filepath\": filepath\n",
    "        }\n",
    "    \n",
    "    def _determine_chart_type(self, df: pd.DataFrame, query: str) -> str:\n",
    "        \"\"\"Use LLM to determine best chart type\"\"\"\n",
    "        data_info = f\"Columns: {list(df.columns)}, Rows: {len(df)}, Types: {df.dtypes.to_dict()}\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Given this data and query, suggest ONE chart type.\n",
    "                DATA INFO: {data_info}\n",
    "                QUERY: {query}\n",
    "                Choose from: bar, line, pie, scatter\n",
    "                Return ONLY the chart type, nothing else:\"\"\")\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format(data_info=data_info, query=query))\n",
    "        \n",
    "        chart_type = response.content.strip().lower()\n",
    "        return chart_type if chart_type in ['bar', 'line', 'pie', 'scatter'] else 'bar'\n",
    "    \n",
    "    def _create_bar_chart(self, df, ax):\n",
    "        \"\"\"Create bar chart\"\"\"\n",
    "        # Use first column as x, second as y\n",
    "        if len(df.columns) >= 2:\n",
    "            x_col, y_col = df.columns[0], df.columns[1]\n",
    "            df_plot = df.head(10)  # Limit to top 10\n",
    "            ax.bar(range(len(df_plot)), df_plot[y_col], color=self.style['colors'][0])\n",
    "            ax.set_xticks(range(len(df_plot)))\n",
    "            ax.set_xticklabels(df_plot[x_col], rotation=45, ha='right')\n",
    "            ax.set_xlabel(x_col)\n",
    "            ax.set_ylabel(y_col)\n",
    "    \n",
    "    def _create_line_chart(self, df, ax):\n",
    "        \"\"\"Create line chart\"\"\"\n",
    "        if len(df.columns) >= 2:\n",
    "            x_col, y_col = df.columns[0], df.columns[1]\n",
    "            ax.plot(df[x_col], df[y_col], marker='o', color=self.style['colors'][0], linewidth=2)\n",
    "            ax.set_xlabel(x_col)\n",
    "            ax.set_ylabel(y_col)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _create_pie_chart(self, df, ax):\n",
    "        \"\"\"Create pie chart\"\"\"\n",
    "        if len(df.columns) >= 2:\n",
    "            labels_col, values_col = df.columns[0], df.columns[1]\n",
    "            df_plot = df.head(10)\n",
    "            ax.pie(df_plot[values_col], labels=df_plot[labels_col], autopct='%1.1f%%',\n",
    "                   colors=self.style['colors'])\n",
    "    \n",
    "    def _create_scatter_plot(self, df, ax):\n",
    "        \"\"\"Create scatter plot\"\"\"\n",
    "        if len(df.columns) >= 2:\n",
    "            x_col, y_col = df.columns[0], df.columns[1]\n",
    "            ax.scatter(df[x_col], df[y_col], alpha=0.6, color=self.style['colors'][0])\n",
    "            ax.set_xlabel(x_col)\n",
    "            ax.set_ylabel(y_col)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _apply_branding(self, ax, query):\n",
    "        \"\"\"Apply company branding\"\"\"\n",
    "        # Title based on query (simplified)\n",
    "        title = query[:50] + \"...\" if len(query) > 50 else query\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Company watermark\n",
    "        ax.text(0.99, 0.01, 'Company Analytics ©', \n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8, alpha=0.5,\n",
    "                ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_agent = VisualizationAgent(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_workflow(query, user_role=\"analyst\"):\n",
    "    \"\"\"Run complete analysis + visualization workflow\"\"\"\n",
    "    \n",
    "    # Define permissions (simulated)\n",
    "    permissions = {\n",
    "        \"analyst\": [\"artists\", \"albums\", \"tracks\", \"invoices\", \"customers\"],\n",
    "        \"viewer\": [\"artists\", \"albums\", \"tracks\"],\n",
    "        \"admin\": None  # all tables\n",
    "    }\n",
    "    \n",
    "    allowed_tables = permissions.get(user_role)\n",
    "    \n",
    "    analysis_result = analysis_agent.analyze(query, allowed_tables)\n",
    "    \n",
    "    if analysis_result[\"status\"] == \"success\":\n",
    "        print(analysis_result[\"insights\"])\n",
    "        print(f\"\\nData shape: {analysis_result['data'].shape}\")\n",
    "        display(analysis_result[\"data\"].head())\n",
    "        \n",
    "        viz_result = viz_agent.visualize(analysis_result[\"data\"], query)\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": analysis_result,\n",
    "            \"visualization\": viz_result\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Error: {analysis_result.get('error')}\")\n",
    "        return analysis_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST 1: Top 10 Selling Artists\")\n",
    "result1 = run_complete_workflow(\n",
    "    \"Show me the top 10 artists by total sales amount\",\n",
    "    user_role=\"analyst\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
