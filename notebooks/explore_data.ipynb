{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from utils import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Database: ../data/chinook.db\n",
      "üìã Tables found: 13\n",
      "\n",
      "  ‚Ä¢ albums: AlbumId, Title, ArtistId\n",
      "  ‚Ä¢ sqlite_sequence: name, seq\n",
      "  ‚Ä¢ artists: ArtistId, Name\n",
      "  ‚Ä¢ customers: CustomerId, FirstName, LastName, Company, Address...\n",
      "  ‚Ä¢ employees: EmployeeId, LastName, FirstName, Title, ReportsTo...\n",
      "  ‚Ä¢ genres: GenreId, Name\n",
      "  ‚Ä¢ invoices: InvoiceId, CustomerId, InvoiceDate, BillingAddress, BillingCity...\n",
      "  ‚Ä¢ invoice_items: InvoiceLineId, InvoiceId, TrackId, UnitPrice, Quantity\n",
      "  ‚Ä¢ media_types: MediaTypeId, Name\n",
      "  ‚Ä¢ playlists: PlaylistId, Name\n",
      "  ‚Ä¢ playlist_track: PlaylistId, TrackId\n",
      "  ‚Ä¢ tracks: TrackId, Name, AlbumId, MediaTypeId, GenreId...\n",
      "  ‚Ä¢ sqlite_stat1: tbl, idx, stat\n"
     ]
    }
   ],
   "source": [
    "def explore_database(db_path):\n",
    "    \"\"\"Explore database schema\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"üìä Database: {db_path}\")\n",
    "    print(f\"üìã Tables found: {len(tables)}\\n\")\n",
    "    \n",
    "    schema = {}\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = cursor.fetchall()\n",
    "        schema[table] = [col[1] for col in columns]\n",
    "        print(f\"  ‚Ä¢ {table}: {', '.join(schema[table][:5])}{'...' if len(schema[table]) > 5 else ''}\")\n",
    "    \n",
    "    conn.close()\n",
    "    return schema\n",
    "\n",
    "# Explore chinook database\n",
    "schema = explore_database(\"../data/chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_str = str(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_usage=calculate_context_percentage(schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 288,\n",
       " 'percentage': 0.22499999999999998,\n",
       " 'context_window': 128000,\n",
       " 'remaining': 127712,\n",
       " 'fits': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"OPEENAI_API_KEY\")\n",
    "model = 'o4-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"https://chat.int.bayer.com/api/v2\",\n",
    "    openai_api_key=token,\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataAnalysisAgent:\n",
    "    \"\"\"Analyzes data using LLM-generated SQL\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path, llm, schema):\n",
    "        self.db_path = db_path\n",
    "        self.llm = llm\n",
    "        self.schema = schema\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        \n",
    "    def analyze(self, user_query: str, allowed_tables: list = None) -> dict:\n",
    "        \"\"\"Main analysis method\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç Query: {user_query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Step 1: Generate SQL\n",
    "        sql_query = self._generate_sql(user_query, allowed_tables)\n",
    "        print(f\"\\nüíª Generated SQL:\\n{sql_query}\\n\")\n",
    "        \n",
    "        # Step 2: Execute with safety checks\n",
    "        if self._is_safe_query(sql_query):\n",
    "            try:\n",
    "                df = pd.read_sql_query(sql_query, self.conn)\n",
    "                print(f\"‚úÖ Query executed: {len(df)} rows returned\\n\")\n",
    "                \n",
    "                # Step 3: Generate insights\n",
    "                insights = self._generate_insights(user_query, df)\n",
    "                \n",
    "                return {\n",
    "                    \"status\": \"success\",\n",
    "                    \"query\": user_query,\n",
    "                    \"sql\": sql_query,\n",
    "                    \"data\": df,\n",
    "                    \"insights\": insights,\n",
    "                    \"rows\": len(df)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e),\n",
    "                    \"query\": user_query\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"Unsafe query detected\",\n",
    "                \"query\": user_query\n",
    "            }\n",
    "    \n",
    "    def _generate_sql(self, user_query: str, allowed_tables: list = None) -> str:\n",
    "        \"\"\"Generate SQL from natural language\"\"\"\n",
    "        tables_info = \"\\n\".join([\n",
    "            f\"- {table}: {', '.join(cols)}\"\n",
    "            for table, cols in self.schema.items()\n",
    "            if allowed_tables is None or table in allowed_tables\n",
    "        ])\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"You are an expert SQL generator for SQLite databases.           \n",
    "                DATABASE SCHEMA:\n",
    "                {schema}\n",
    "                USER QUERY: {query}\n",
    "                Generate a safe, efficient SELECT query. Rules:\n",
    "                1. ONLY use SELECT statements (no INSERT, UPDATE, DELETE, DROP)\n",
    "                2. Include LIMIT clause if not specified (default LIMIT 100)\n",
    "                3. Use proper JOINs when needed\n",
    "                4. Return ONLY the SQL query, no explanations\n",
    "                SQL Query:\"\"\")\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format(schema=tables_info, query=user_query))\n",
    "        \n",
    "        # Clean the response\n",
    "        sql = response.content.strip()\n",
    "        # Remove markdown code blocks if present\n",
    "        sql = sql.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        return sql\n",
    "    \n",
    "    def _is_safe_query(self, sql: str) -> bool:\n",
    "        \"\"\"Check if SQL query is safe\"\"\"\n",
    "        sql_upper = sql.upper()\n",
    "        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']\n",
    "        \n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in sql_upper:\n",
    "                print(f\"‚ö†Ô∏è  Dangerous keyword detected: {keyword}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _generate_insights(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate natural language insights\"\"\"\n",
    "        data_summary = df.head(10).to_string()\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Based on this query and results, provide 2-3 key insights in bullet points.\n",
    "                QUERY: {query}\n",
    "                RESULTS (first 10 rows):\n",
    "                {data}\n",
    "                Key Insights (2-3 bullets):\"\"\")\n",
    "        \n",
    "        response = self.llm.invoke(prompt.format(query=query, data=data_summary))\n",
    "        \n",
    "        return response.content.strip()\n",
    "    \n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "\n",
    "analysis_agent = DataAnalysisAgent(\"../data/chinook.db\", llm, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç Query: Give me artist with most albulmns\n",
      "============================================================\n",
      "\n",
      "üíª Generated SQL:\n",
      "SELECT a.Name, COUNT(al.AlbumId) AS album_count\n",
      "FROM artists AS a\n",
      "JOIN albums AS al ON a.ArtistId = al.ArtistId\n",
      "GROUP BY a.ArtistId\n",
      "ORDER BY album_count DESC\n",
      "LIMIT 1;\n",
      "\n",
      "‚úÖ Query executed: 1 rows returned\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'query': 'Give me artist with most albulmns',\n",
       " 'sql': 'SELECT a.Name, COUNT(al.AlbumId) AS album_count\\nFROM artists AS a\\nJOIN albums AS al ON a.ArtistId = al.ArtistId\\nGROUP BY a.ArtistId\\nORDER BY album_count DESC\\nLIMIT 1;',\n",
       " 'data':           Name  album_count\n",
       " 0  Iron Maiden           21,\n",
       " 'insights': '‚Ä¢ Iron Maiden tops the list with 21 albums, making them the single most prolific artist in this dataset.  \\n‚Ä¢ Their 21-album output stands out sharply against all other artists, marking them as a clear outlier.  \\n‚Ä¢ This level of consistency and longevity suggests Iron Maiden‚Äôs enduring productivity over multiple decades.',\n",
       " 'rows': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_agent.analyze(\"Give me artist with most albulmns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
